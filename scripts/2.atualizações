1. Conectar
2. Ler
3. ajustar
4. Transferir/fundir/fazer a ponte
5. Fechar.


Atualizando os métodos de importação para verificar se a fonte é um arquivo local ou uma url.

É importante sempre criar uma base de dados nova como backup. Uma atualização futura pode criar um banco de dados
adicionando um caractere a ele caso ele já exista. 

A ultima coisa feita foi testar a parte de identificação do arquivo, 
que deu erro. O primeiro passo é concerta-lo

Transformar o local do arquivo de windows para a url do ambiente virutal;
"/mnt/c/Users/Gabriel/Desktop/studyprojects/pomodorojson.json"

Todas as funções de leitura e interpretação de dados LOCAIS foram executadas e tudo está funcional.

Testando funções de leitura e intrepretação de dados externo .

Você passou o tempo todo fazendo commito desses arquivos. Agora é necessario copiar o "pomodoro.json" para dentro dessa pasta
relizar o commit
criar um novo banco de dados mongo e usar a biblioteca requests para transferir os arquivos.

Todos os "analisadores" de fonte estão funcionando tanto para locais quanto para arquivos online. 

Criar o banco de dados no mongo e adicionar a coleção 

Alterada a classe de operações. Ela agora necessita da classe de conecção pois a criação de bancos de dados
requisita o estanciamento.


CLASSE LOAD DATA
Agora é necessário salvar a data importada online no banco de dados Mongo. O que estava acontecendo
era um problema de conversão. Continue a pesquisa partindo da ideia de unificação "Mundial" de dados, onde
não importa o formato no qual o dado é importado, e sem armazenado e uma variável vazia ou CSV ou JSON. 

A lógica é : Lê>
append em string
converte no formato
insert no banco de dados

O bloco JSON de execução 

response = requests.get(self.data_url)
data_list = [response.text]  # Cria uma lista contendo o texto da resposta
conteudo_json = json.loads(data_list[0])  # Carrega o conteúdo JSON do texto
Está funcional 

O método de inserir JSON a partir de links online está funcionando, contudo há um problema na hora de redefinir
o atributo da classe (data_type) com o Data Handler. Isso está prejudicando a interpretação do carregador
de dados

O método de inserir json a partir de links online foi testado e está funcional, agora é necessário 
trabalhar o método para arquivos locais.

O código que lê arquivos locais em Json foi testado e está funcional, agora preciso testar para CSV

Detector de CSV testado e declarado como funcional.

Agora programar o leitor de CSV. O leitor de CSV esta funcional. Adicionada a ideia de identificação de tipo
de arquivo para arquivos online. Agora o resultado da requisição é analisado e segregado entre CSV ou JSON
para garantir a correta impressão dele em load_Data.

Agora é necessário criar toda a lógica de migração para o mySQL. Fora previamente determinado a divisão entre
classes de conexão e de operações. As classes de Mongo e MySQL devem ficar separadas. 

A lógica de migração de dados foi criada e implementada, agora precisa ser testada. 
O primeiro teste a ser realizado é o de garantir a conexão com o mySQL/cursor do mysql. 

Como esperado a conexão não está sendo efetuada. Agora preciso entender o por que. 
Eu digitei a senha "C5" no campo de senhas daí foi 

O próximo passo é verificar a classe de manejamento de dados mysql, seu instanciamento e por fim como
ela recebe/retorna dados. Após isso, testar função de migração. 

Funcionamento da instancia da classe de operações paa o mySQL definida. Buscar agora efetuar o pipeline de 
extração e migração dos dados.


